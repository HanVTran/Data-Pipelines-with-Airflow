https://sonra.io/2018/01/01/using-apache-airflow-to-build-a-data-pipeline-on-aws/
In this section we will cover:

Installation of Airflow
Defining a database connection using Airflow
Developing the parameterizable S3 to Redshift operator
Developing the re-usable Redshift Upsert operator
Deployment operations

==================================

http://dwgeek.com/run-redshift-sql-script-file-using-psql-variable-substitution.html/

Run Redshift SQL Script File using psql Variable Substitution

====================================

http://dwgeek.com/steps-connect-redshift-cluster-using-postgresql-psql.html/

Connect to Redshift cluster using PostgreSQL â€“ psql
To connect to Redshift from psql, you must specify the cluster endpoint (your cluster name), database, port and provide password at the run-time. At a command prompt, specify the connection information by using either command line parameters or a connection information string.

psql -h <endpoint> -U <userid> -d <databasename> -p <port>
Where:

<endpoint> It is nothing but Redshift cluster name.
<userid> is a user ID with permissions to connect to the cluster.
<databasename> is the Database name to which you want to connect.
<port> is port name. Default port is, 5439
Now, you are all set to connect to Redshift from psql. Go ahead and provide all require parameter values.
